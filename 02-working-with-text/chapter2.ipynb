{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbafb550-fdda-4eb3-b49b-b39f80c98cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the sample file and print a few things based on what was read\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c640f0-2120-4cf0-9257-4134075204d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize based on whitespace\n",
    "\n",
    "import re\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d02b7fb-1ea3-497e-94e4-8fa78406c67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include punctuation as tokens\n",
    "result = re.split(r'([,.]|\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c7eb4f-435b-4db6-a511-943dbdbd7eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now remove the spaces - note for some models we'll keep the spaces as they can be significat, e.g. python syntax\n",
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c77f06-acaf-45e6-b041-ca3cfdd97711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand to include addition punctuation, etc\n",
    "\n",
    "text = \"Hello, world. Is this-- a test?\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea2e592-5501-46de-8c29-1c237b067292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the example text\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2296cbb5-7a80-480a-9db4-908574a9b77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fb9cd5-5a14-4f3d-a146-601aecd8bb7f",
   "metadata": {},
   "source": [
    "### Building a vocabulary\n",
    "\n",
    "A vocabulary defines how we map each word and special character to a unique identifier. To build a vocabulary we tokenize the entire dataset, sort it, and remove duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d930f87f-e87e-4d22-91ea-7058e60862d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates and sort the tokens from our tokenized dataset\n",
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce716da5-a1e5-485a-b2d8-890049c62633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vocabulary from the sorted/deduplicated tokens\n",
    "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9098937a-b0af-41fd-8a04-438976ea46ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "enumerate(all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e00aec-e9c0-4315-8f5e-3a60100f2a84",
   "metadata": {},
   "source": [
    "Note in addition to mapping tokens to integers, we want to take the output of LLMs and map integer output to tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37b0930-ac03-4aff-b7b9-56fe4b2fe972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple tokenizer class\n",
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids]) \n",
    "\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b729e1c9-957c-4add-9117-5df1349a9472",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = \"\"\"\"It's the last he painted, you know,\" \n",
    "       Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0029fdf7-4fe0-4079-948c-388d115bd5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ff34dc-5d2e-42da-85d5-a2593ccc4396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if the token is not in the vocab? We get a KeyError. Having a large dataset can mitigate this somewhat...\n",
    "text = \"Hello, do you like tea?\"\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73c986d-b111-495f-ba32-f9496ac92d27",
   "metadata": {},
   "source": [
    "## Special Context Tokens\n",
    "\n",
    "Modify the tokenizer to handle unknown words, and address the usage and addition of special context tokens that can enhance a model's understanding of context or other relevent information in the text. \n",
    "\n",
    "Below we'll use <|unk|> to represent unknown words, and <|endoftext|> to signal text following the token presents a new text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f68fe9-0cb8-4d15-8ab8-a5c4ecf73cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the tokens\n",
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
    "\n",
    "print(len(vocab.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8f19bd-1fa0-49a5-86c4-d1de6a681a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated tokenizer that can deal with unknown words\n",
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        preprocessed = [item if item in self.str_to_int\n",
    "                        else \"<|unk|>\" for item in preprocessed]\n",
    "\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69d4b21-aba0-4b3e-9cb7-9ba4b808fe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b2d3b5-4ea6-49ee-8b1c-12fb98bf9006",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1097f6-0415-4277-bf2d-0051f7a2aaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b208bd-6dfa-49e2-bf6e-3d3cb589d171",
   "metadata": {},
   "source": [
    "## Byte Pair Encoding Tokenizer\n",
    "\n",
    "BPE tokenizers break down unknown words into subwords and individual characters. This way, a BPE tokenizer can parse any word and doesn’t need to replace unknown words with special tokens, such as <|unk|>.\n",
    "\n",
    "Both GPT-2 and GPT-3 used this tokenizer algorithm.\n",
    "\n",
    "Implementation library - [tiktoken](https://github.com/openai/tiktoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e7babb-993e-4d6c-b601-0e3c722876e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.metadata import version\n",
    "import tiktoken\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa4259c-3fdb-45a5-bfcf-5861e812df9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aede52aa-08ed-44ce-84e5-41d10d761668",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "     \"of someunknownPlace.\"\n",
    ")\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d164bf85-3ba8-4d3e-b3e6-dcfab3780d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d153826-5cfe-43cc-963c-3a2742fa8180",
   "metadata": {},
   "source": [
    "## Data Sampling with Sliding Window\n",
    "\n",
    "The next step in creating the embeddings for the LLM is to generate the input–target pairs required for training an LLM. What do these input–target pairs look like? As we already learned, LLMs are pretrained by predicting the next word in a text.\n",
    "\n",
    "Consider a text sample \"LLMS learn to predict one word at a time\".\n",
    "\n",
    "Pairings for training using sliding windows are \\[LLMs\\]\\[learn\\],  \\[LLMs learn\\]\\[to\\],  \\[LLMs learn to\\]\\[predict\\], and so on,\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1981f259-a806-4611-8b38-f80fb96067e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the input using the BPE tokenizer\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86334159-ad45-485e-a758-7b13f9c0af4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the first 50 tokens from the dataset - doing makes for a better demo\n",
    "enc_sample = enc_text[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a426e5-0733-46c2-b177-93f4c7afcaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the input–target pairs for the next-word prediction task is to create two variables, x and y, \n",
    "# where x contains the input tokens and y contains the targets, which are the inputs shifted by 1:\n",
    "context_size = 4\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a81af1-cb32-432f-bad1-d5fac2b84c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(context, \"---->\", desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de27d282-c548-450b-a699-3d7347d52f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same thing, except decode the token ids\n",
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910347ed-3fcb-4bba-b423-433cfdfa6d37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
